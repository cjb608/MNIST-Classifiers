{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xucW8qxvRq4w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load and Prepare the MNIST Dataset"
      ],
      "metadata": {
        "id": "BlI21KpKSQDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize function"
      ],
      "metadata": {
        "id": "4XekPtrySgCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(data):\n",
        "  return np.reshape(data, (1, -1))"
      ],
      "metadata": {
        "id": "xRpobDoSSip_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalize function"
      ],
      "metadata": {
        "id": "vRE0xnOQSsUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset and split it into training and test sets\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Vectorize the data\n",
        "x_train = np.squeeze(np.array([vectorize(i) for i in x_train]))\n",
        "x_test = np.squeeze(np.array([vectorize(i) for i in x_test]))\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "x_train = scaler.transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "\n",
        "# Add a constant feature for bias\n",
        "x_train = np.hstack((np.ones((len(x_train),1)), x_train))\n",
        "x_test = np.hstack((np.ones((len(x_test),1)), x_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_8nLCYJTZWo",
        "outputId": "f86e24c7-d3aa-4610-c07d-b9006b49c0f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multinonmial Logistic Regression Model"
      ],
      "metadata": {
        "id": "eR5BO4dQUa9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialLogisticRegression():\n",
        "  def __init__(self, lr=0.1, max_iter=1000, tol=0.01):\n",
        "    self.lr = lr\n",
        "    self.max_iter = max_iter\n",
        "    self.tol = tol\n",
        "    self.w = []\n",
        "\n",
        "  def one_hot(self, y):\n",
        "    num_classes = np.shape(np.unique(y))[0]\n",
        "    y_one_hot = np.zeros((len(y), num_classes))\n",
        "    for i in range(len(y)):\n",
        "      y_one_hot[i, y[i]] = 1\n",
        "    return y_one_hot\n",
        "\n",
        "  def weight_init(self, num_features, num_classes):\n",
        "    self.w = np.zeros((num_features, num_classes))\n",
        "\n",
        "  def softmax(self, z):\n",
        "    probs = np.exp(z) / np.sum(np.exp(z), axis=1).reshape(-1, 1)\n",
        "    return probs\n",
        "\n",
        "  def forward_pass(self, x):\n",
        "    z = np.dot(x, self.w)\n",
        "    y_hat_probs = self.softmax(z)\n",
        "    return y_hat_probs\n",
        "\n",
        "  def log_loss(self, y_one_hot, y_hat_probs):\n",
        "    loss = -1 / (len(y_one_hot)) * np.sum(y_one_hot * np.log(y_hat_probs))\n",
        "    return loss\n",
        "\n",
        "  def accuracy(self, y, y_hat_probs):\n",
        "    y_hat = np.argmax(y_hat_probs, axis=1)\n",
        "    acc = 1.0 - np.count_nonzero(y_hat - y) / len(y)\n",
        "    return acc\n",
        "\n",
        "  def backward_pass(self, x, y_one_hot, y_hat_probs):\n",
        "    dw = -1 / len(y_one_hot) * np.dot(x.T, (y_one_hot - y_hat_probs))\n",
        "    return dw\n",
        "\n",
        "  def fit(self, x, y):\n",
        "    i = 0\n",
        "    loss = 1\n",
        "    y_one_hot = self.one_hot(y)\n",
        "    self.weight_init(np.shape(x)[1], np.shape(y_one_hot)[1])\n",
        "    while i < self.max_iter and loss > self.tol:\n",
        "      y_hat_probs = self.forward_pass(x)\n",
        "      loss = self.log_loss(y_one_hot, y_hat_probs)\n",
        "      acc = self.accuracy(y, y_hat_probs)\n",
        "      dw = self.backward_pass(x, y_one_hot, y_hat_probs)\n",
        "      self.w -= self.lr * dw\n",
        "      i += 1\n",
        "      print('Iteration {}: loss = {:.4f}, accuracy = {:.4f}'.format(i, loss, acc))\n",
        "\n",
        "  def predict(self, x):\n",
        "    y_hat_probs = self.forward_pass(x)\n",
        "    y_hat = np.argmax(y_hat_probs, axis=1)\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "vRNnLMDrG1GT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialLogisticRegression(lr=0.2, max_iter=500, tol=0.01)"
      ],
      "metadata": {
        "id": "1bjd0e_AM-_u"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM1SaUE9NPih",
        "outputId": "64b3aeba-c1d6-4267-f070-84842ad78199"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: loss = 2.3026, accuracy = 0.0987\n",
            "Iteration 2: loss = 1.1608, accuracy = 0.7361\n",
            "Iteration 3: loss = 0.8864, accuracy = 0.7949\n",
            "Iteration 4: loss = 0.7591, accuracy = 0.8230\n",
            "Iteration 5: loss = 0.6819, accuracy = 0.8398\n",
            "Iteration 6: loss = 0.6293, accuracy = 0.8501\n",
            "Iteration 7: loss = 0.5908, accuracy = 0.8576\n",
            "Iteration 8: loss = 0.5612, accuracy = 0.8632\n",
            "Iteration 9: loss = 0.5376, accuracy = 0.8672\n",
            "Iteration 10: loss = 0.5182, accuracy = 0.8709\n",
            "Iteration 11: loss = 0.5019, accuracy = 0.8739\n",
            "Iteration 12: loss = 0.4881, accuracy = 0.8761\n",
            "Iteration 13: loss = 0.4761, accuracy = 0.8783\n",
            "Iteration 14: loss = 0.4656, accuracy = 0.8803\n",
            "Iteration 15: loss = 0.4563, accuracy = 0.8821\n",
            "Iteration 16: loss = 0.4480, accuracy = 0.8836\n",
            "Iteration 17: loss = 0.4405, accuracy = 0.8849\n",
            "Iteration 18: loss = 0.4338, accuracy = 0.8864\n",
            "Iteration 19: loss = 0.4276, accuracy = 0.8877\n",
            "Iteration 20: loss = 0.4219, accuracy = 0.8889\n",
            "Iteration 21: loss = 0.4168, accuracy = 0.8903\n",
            "Iteration 22: loss = 0.4120, accuracy = 0.8911\n",
            "Iteration 23: loss = 0.4075, accuracy = 0.8917\n",
            "Iteration 24: loss = 0.4034, accuracy = 0.8925\n",
            "Iteration 25: loss = 0.3995, accuracy = 0.8932\n",
            "Iteration 26: loss = 0.3959, accuracy = 0.8937\n",
            "Iteration 27: loss = 0.3924, accuracy = 0.8944\n",
            "Iteration 28: loss = 0.3892, accuracy = 0.8949\n",
            "Iteration 29: loss = 0.3862, accuracy = 0.8956\n",
            "Iteration 30: loss = 0.3833, accuracy = 0.8961\n",
            "Iteration 31: loss = 0.3806, accuracy = 0.8969\n",
            "Iteration 32: loss = 0.3781, accuracy = 0.8974\n",
            "Iteration 33: loss = 0.3756, accuracy = 0.8978\n",
            "Iteration 34: loss = 0.3733, accuracy = 0.8983\n",
            "Iteration 35: loss = 0.3710, accuracy = 0.8990\n",
            "Iteration 36: loss = 0.3689, accuracy = 0.8995\n",
            "Iteration 37: loss = 0.3669, accuracy = 0.9000\n",
            "Iteration 38: loss = 0.3649, accuracy = 0.9003\n",
            "Iteration 39: loss = 0.3631, accuracy = 0.9005\n",
            "Iteration 40: loss = 0.3613, accuracy = 0.9008\n",
            "Iteration 41: loss = 0.3595, accuracy = 0.9012\n",
            "Iteration 42: loss = 0.3579, accuracy = 0.9015\n",
            "Iteration 43: loss = 0.3563, accuracy = 0.9019\n",
            "Iteration 44: loss = 0.3547, accuracy = 0.9022\n",
            "Iteration 45: loss = 0.3532, accuracy = 0.9025\n",
            "Iteration 46: loss = 0.3518, accuracy = 0.9027\n",
            "Iteration 47: loss = 0.3504, accuracy = 0.9031\n",
            "Iteration 48: loss = 0.3491, accuracy = 0.9033\n",
            "Iteration 49: loss = 0.3478, accuracy = 0.9037\n",
            "Iteration 50: loss = 0.3465, accuracy = 0.9038\n",
            "Iteration 51: loss = 0.3453, accuracy = 0.9040\n",
            "Iteration 52: loss = 0.3441, accuracy = 0.9043\n",
            "Iteration 53: loss = 0.3429, accuracy = 0.9046\n",
            "Iteration 54: loss = 0.3418, accuracy = 0.9049\n",
            "Iteration 55: loss = 0.3407, accuracy = 0.9051\n",
            "Iteration 56: loss = 0.3397, accuracy = 0.9054\n",
            "Iteration 57: loss = 0.3386, accuracy = 0.9056\n",
            "Iteration 58: loss = 0.3376, accuracy = 0.9058\n",
            "Iteration 59: loss = 0.3367, accuracy = 0.9061\n",
            "Iteration 60: loss = 0.3357, accuracy = 0.9064\n",
            "Iteration 61: loss = 0.3348, accuracy = 0.9066\n",
            "Iteration 62: loss = 0.3339, accuracy = 0.9068\n",
            "Iteration 63: loss = 0.3330, accuracy = 0.9070\n",
            "Iteration 64: loss = 0.3322, accuracy = 0.9072\n",
            "Iteration 65: loss = 0.3313, accuracy = 0.9074\n",
            "Iteration 66: loss = 0.3305, accuracy = 0.9075\n",
            "Iteration 67: loss = 0.3297, accuracy = 0.9077\n",
            "Iteration 68: loss = 0.3289, accuracy = 0.9079\n",
            "Iteration 69: loss = 0.3282, accuracy = 0.9081\n",
            "Iteration 70: loss = 0.3274, accuracy = 0.9084\n",
            "Iteration 71: loss = 0.3267, accuracy = 0.9086\n",
            "Iteration 72: loss = 0.3260, accuracy = 0.9088\n",
            "Iteration 73: loss = 0.3253, accuracy = 0.9090\n",
            "Iteration 74: loss = 0.3246, accuracy = 0.9092\n",
            "Iteration 75: loss = 0.3239, accuracy = 0.9094\n",
            "Iteration 76: loss = 0.3233, accuracy = 0.9095\n",
            "Iteration 77: loss = 0.3226, accuracy = 0.9097\n",
            "Iteration 78: loss = 0.3220, accuracy = 0.9098\n",
            "Iteration 79: loss = 0.3214, accuracy = 0.9100\n",
            "Iteration 80: loss = 0.3208, accuracy = 0.9101\n",
            "Iteration 81: loss = 0.3202, accuracy = 0.9103\n",
            "Iteration 82: loss = 0.3196, accuracy = 0.9105\n",
            "Iteration 83: loss = 0.3190, accuracy = 0.9106\n",
            "Iteration 84: loss = 0.3184, accuracy = 0.9108\n",
            "Iteration 85: loss = 0.3179, accuracy = 0.9109\n",
            "Iteration 86: loss = 0.3173, accuracy = 0.9110\n",
            "Iteration 87: loss = 0.3168, accuracy = 0.9112\n",
            "Iteration 88: loss = 0.3163, accuracy = 0.9112\n",
            "Iteration 89: loss = 0.3158, accuracy = 0.9113\n",
            "Iteration 90: loss = 0.3153, accuracy = 0.9115\n",
            "Iteration 91: loss = 0.3148, accuracy = 0.9118\n",
            "Iteration 92: loss = 0.3143, accuracy = 0.9119\n",
            "Iteration 93: loss = 0.3138, accuracy = 0.9121\n",
            "Iteration 94: loss = 0.3133, accuracy = 0.9121\n",
            "Iteration 95: loss = 0.3128, accuracy = 0.9122\n",
            "Iteration 96: loss = 0.3124, accuracy = 0.9124\n",
            "Iteration 97: loss = 0.3119, accuracy = 0.9125\n",
            "Iteration 98: loss = 0.3115, accuracy = 0.9126\n",
            "Iteration 99: loss = 0.3110, accuracy = 0.9128\n",
            "Iteration 100: loss = 0.3106, accuracy = 0.9129\n",
            "Iteration 101: loss = 0.3102, accuracy = 0.9130\n",
            "Iteration 102: loss = 0.3098, accuracy = 0.9130\n",
            "Iteration 103: loss = 0.3093, accuracy = 0.9132\n",
            "Iteration 104: loss = 0.3089, accuracy = 0.9133\n",
            "Iteration 105: loss = 0.3085, accuracy = 0.9134\n",
            "Iteration 106: loss = 0.3081, accuracy = 0.9135\n",
            "Iteration 107: loss = 0.3077, accuracy = 0.9136\n",
            "Iteration 108: loss = 0.3074, accuracy = 0.9136\n",
            "Iteration 109: loss = 0.3070, accuracy = 0.9137\n",
            "Iteration 110: loss = 0.3066, accuracy = 0.9138\n",
            "Iteration 111: loss = 0.3062, accuracy = 0.9138\n",
            "Iteration 112: loss = 0.3059, accuracy = 0.9140\n",
            "Iteration 113: loss = 0.3055, accuracy = 0.9141\n",
            "Iteration 114: loss = 0.3052, accuracy = 0.9142\n",
            "Iteration 115: loss = 0.3048, accuracy = 0.9143\n",
            "Iteration 116: loss = 0.3045, accuracy = 0.9144\n",
            "Iteration 117: loss = 0.3041, accuracy = 0.9145\n",
            "Iteration 118: loss = 0.3038, accuracy = 0.9145\n",
            "Iteration 119: loss = 0.3035, accuracy = 0.9146\n",
            "Iteration 120: loss = 0.3031, accuracy = 0.9148\n",
            "Iteration 121: loss = 0.3028, accuracy = 0.9149\n",
            "Iteration 122: loss = 0.3025, accuracy = 0.9150\n",
            "Iteration 123: loss = 0.3022, accuracy = 0.9152\n",
            "Iteration 124: loss = 0.3018, accuracy = 0.9152\n",
            "Iteration 125: loss = 0.3015, accuracy = 0.9153\n",
            "Iteration 126: loss = 0.3012, accuracy = 0.9154\n",
            "Iteration 127: loss = 0.3009, accuracy = 0.9154\n",
            "Iteration 128: loss = 0.3006, accuracy = 0.9154\n",
            "Iteration 129: loss = 0.3003, accuracy = 0.9155\n",
            "Iteration 130: loss = 0.3000, accuracy = 0.9156\n",
            "Iteration 131: loss = 0.2998, accuracy = 0.9158\n",
            "Iteration 132: loss = 0.2995, accuracy = 0.9158\n",
            "Iteration 133: loss = 0.2992, accuracy = 0.9159\n",
            "Iteration 134: loss = 0.2989, accuracy = 0.9158\n",
            "Iteration 135: loss = 0.2986, accuracy = 0.9159\n",
            "Iteration 136: loss = 0.2984, accuracy = 0.9160\n",
            "Iteration 137: loss = 0.2981, accuracy = 0.9161\n",
            "Iteration 138: loss = 0.2978, accuracy = 0.9163\n",
            "Iteration 139: loss = 0.2976, accuracy = 0.9163\n",
            "Iteration 140: loss = 0.2973, accuracy = 0.9163\n",
            "Iteration 141: loss = 0.2970, accuracy = 0.9164\n",
            "Iteration 142: loss = 0.2968, accuracy = 0.9164\n",
            "Iteration 143: loss = 0.2965, accuracy = 0.9165\n",
            "Iteration 144: loss = 0.2963, accuracy = 0.9166\n",
            "Iteration 145: loss = 0.2960, accuracy = 0.9167\n",
            "Iteration 146: loss = 0.2958, accuracy = 0.9168\n",
            "Iteration 147: loss = 0.2955, accuracy = 0.9170\n",
            "Iteration 148: loss = 0.2953, accuracy = 0.9171\n",
            "Iteration 149: loss = 0.2951, accuracy = 0.9171\n",
            "Iteration 150: loss = 0.2948, accuracy = 0.9172\n",
            "Iteration 151: loss = 0.2946, accuracy = 0.9173\n",
            "Iteration 152: loss = 0.2944, accuracy = 0.9173\n",
            "Iteration 153: loss = 0.2941, accuracy = 0.9173\n",
            "Iteration 154: loss = 0.2939, accuracy = 0.9175\n",
            "Iteration 155: loss = 0.2937, accuracy = 0.9176\n",
            "Iteration 156: loss = 0.2935, accuracy = 0.9177\n",
            "Iteration 157: loss = 0.2932, accuracy = 0.9177\n",
            "Iteration 158: loss = 0.2930, accuracy = 0.9178\n",
            "Iteration 159: loss = 0.2928, accuracy = 0.9178\n",
            "Iteration 160: loss = 0.2926, accuracy = 0.9179\n",
            "Iteration 161: loss = 0.2924, accuracy = 0.9179\n",
            "Iteration 162: loss = 0.2922, accuracy = 0.9180\n",
            "Iteration 163: loss = 0.2920, accuracy = 0.9181\n",
            "Iteration 164: loss = 0.2918, accuracy = 0.9182\n",
            "Iteration 165: loss = 0.2916, accuracy = 0.9182\n",
            "Iteration 166: loss = 0.2914, accuracy = 0.9183\n",
            "Iteration 167: loss = 0.2912, accuracy = 0.9184\n",
            "Iteration 168: loss = 0.2910, accuracy = 0.9184\n",
            "Iteration 169: loss = 0.2908, accuracy = 0.9185\n",
            "Iteration 170: loss = 0.2906, accuracy = 0.9185\n",
            "Iteration 171: loss = 0.2904, accuracy = 0.9186\n",
            "Iteration 172: loss = 0.2902, accuracy = 0.9186\n",
            "Iteration 173: loss = 0.2900, accuracy = 0.9187\n",
            "Iteration 174: loss = 0.2898, accuracy = 0.9187\n",
            "Iteration 175: loss = 0.2896, accuracy = 0.9188\n",
            "Iteration 176: loss = 0.2894, accuracy = 0.9188\n",
            "Iteration 177: loss = 0.2892, accuracy = 0.9189\n",
            "Iteration 178: loss = 0.2891, accuracy = 0.9190\n",
            "Iteration 179: loss = 0.2889, accuracy = 0.9190\n",
            "Iteration 180: loss = 0.2887, accuracy = 0.9190\n",
            "Iteration 181: loss = 0.2885, accuracy = 0.9191\n",
            "Iteration 182: loss = 0.2883, accuracy = 0.9191\n",
            "Iteration 183: loss = 0.2882, accuracy = 0.9191\n",
            "Iteration 184: loss = 0.2880, accuracy = 0.9192\n",
            "Iteration 185: loss = 0.2878, accuracy = 0.9192\n",
            "Iteration 186: loss = 0.2876, accuracy = 0.9193\n",
            "Iteration 187: loss = 0.2875, accuracy = 0.9194\n",
            "Iteration 188: loss = 0.2873, accuracy = 0.9194\n",
            "Iteration 189: loss = 0.2871, accuracy = 0.9195\n",
            "Iteration 190: loss = 0.2870, accuracy = 0.9196\n",
            "Iteration 191: loss = 0.2868, accuracy = 0.9196\n",
            "Iteration 192: loss = 0.2866, accuracy = 0.9196\n",
            "Iteration 193: loss = 0.2865, accuracy = 0.9197\n",
            "Iteration 194: loss = 0.2863, accuracy = 0.9198\n",
            "Iteration 195: loss = 0.2862, accuracy = 0.9198\n",
            "Iteration 196: loss = 0.2860, accuracy = 0.9199\n",
            "Iteration 197: loss = 0.2858, accuracy = 0.9199\n",
            "Iteration 198: loss = 0.2857, accuracy = 0.9199\n",
            "Iteration 199: loss = 0.2855, accuracy = 0.9200\n",
            "Iteration 200: loss = 0.2854, accuracy = 0.9200\n",
            "Iteration 201: loss = 0.2852, accuracy = 0.9201\n",
            "Iteration 202: loss = 0.2851, accuracy = 0.9201\n",
            "Iteration 203: loss = 0.2849, accuracy = 0.9202\n",
            "Iteration 204: loss = 0.2848, accuracy = 0.9202\n",
            "Iteration 205: loss = 0.2846, accuracy = 0.9203\n",
            "Iteration 206: loss = 0.2845, accuracy = 0.9203\n",
            "Iteration 207: loss = 0.2843, accuracy = 0.9203\n",
            "Iteration 208: loss = 0.2842, accuracy = 0.9204\n",
            "Iteration 209: loss = 0.2840, accuracy = 0.9204\n",
            "Iteration 210: loss = 0.2839, accuracy = 0.9204\n",
            "Iteration 211: loss = 0.2837, accuracy = 0.9205\n",
            "Iteration 212: loss = 0.2836, accuracy = 0.9205\n",
            "Iteration 213: loss = 0.2835, accuracy = 0.9205\n",
            "Iteration 214: loss = 0.2833, accuracy = 0.9205\n",
            "Iteration 215: loss = 0.2832, accuracy = 0.9206\n",
            "Iteration 216: loss = 0.2830, accuracy = 0.9207\n",
            "Iteration 217: loss = 0.2829, accuracy = 0.9207\n",
            "Iteration 218: loss = 0.2828, accuracy = 0.9206\n",
            "Iteration 219: loss = 0.2826, accuracy = 0.9206\n",
            "Iteration 220: loss = 0.2825, accuracy = 0.9207\n",
            "Iteration 221: loss = 0.2824, accuracy = 0.9208\n",
            "Iteration 222: loss = 0.2822, accuracy = 0.9208\n",
            "Iteration 223: loss = 0.2821, accuracy = 0.9209\n",
            "Iteration 224: loss = 0.2820, accuracy = 0.9210\n",
            "Iteration 225: loss = 0.2818, accuracy = 0.9210\n",
            "Iteration 226: loss = 0.2817, accuracy = 0.9210\n",
            "Iteration 227: loss = 0.2816, accuracy = 0.9211\n",
            "Iteration 228: loss = 0.2814, accuracy = 0.9212\n",
            "Iteration 229: loss = 0.2813, accuracy = 0.9212\n",
            "Iteration 230: loss = 0.2812, accuracy = 0.9212\n",
            "Iteration 231: loss = 0.2811, accuracy = 0.9213\n",
            "Iteration 232: loss = 0.2809, accuracy = 0.9213\n",
            "Iteration 233: loss = 0.2808, accuracy = 0.9214\n",
            "Iteration 234: loss = 0.2807, accuracy = 0.9214\n",
            "Iteration 235: loss = 0.2806, accuracy = 0.9216\n",
            "Iteration 236: loss = 0.2804, accuracy = 0.9216\n",
            "Iteration 237: loss = 0.2803, accuracy = 0.9216\n",
            "Iteration 238: loss = 0.2802, accuracy = 0.9217\n",
            "Iteration 239: loss = 0.2801, accuracy = 0.9217\n",
            "Iteration 240: loss = 0.2800, accuracy = 0.9217\n",
            "Iteration 241: loss = 0.2798, accuracy = 0.9217\n",
            "Iteration 242: loss = 0.2797, accuracy = 0.9217\n",
            "Iteration 243: loss = 0.2796, accuracy = 0.9217\n",
            "Iteration 244: loss = 0.2795, accuracy = 0.9218\n",
            "Iteration 245: loss = 0.2794, accuracy = 0.9219\n",
            "Iteration 246: loss = 0.2793, accuracy = 0.9219\n",
            "Iteration 247: loss = 0.2792, accuracy = 0.9219\n",
            "Iteration 248: loss = 0.2790, accuracy = 0.9219\n",
            "Iteration 249: loss = 0.2789, accuracy = 0.9220\n",
            "Iteration 250: loss = 0.2788, accuracy = 0.9220\n",
            "Iteration 251: loss = 0.2787, accuracy = 0.9220\n",
            "Iteration 252: loss = 0.2786, accuracy = 0.9220\n",
            "Iteration 253: loss = 0.2785, accuracy = 0.9221\n",
            "Iteration 254: loss = 0.2784, accuracy = 0.9221\n",
            "Iteration 255: loss = 0.2783, accuracy = 0.9221\n",
            "Iteration 256: loss = 0.2782, accuracy = 0.9221\n",
            "Iteration 257: loss = 0.2780, accuracy = 0.9221\n",
            "Iteration 258: loss = 0.2779, accuracy = 0.9222\n",
            "Iteration 259: loss = 0.2778, accuracy = 0.9222\n",
            "Iteration 260: loss = 0.2777, accuracy = 0.9222\n",
            "Iteration 261: loss = 0.2776, accuracy = 0.9223\n",
            "Iteration 262: loss = 0.2775, accuracy = 0.9223\n",
            "Iteration 263: loss = 0.2774, accuracy = 0.9223\n",
            "Iteration 264: loss = 0.2773, accuracy = 0.9224\n",
            "Iteration 265: loss = 0.2772, accuracy = 0.9224\n",
            "Iteration 266: loss = 0.2771, accuracy = 0.9224\n",
            "Iteration 267: loss = 0.2770, accuracy = 0.9224\n",
            "Iteration 268: loss = 0.2769, accuracy = 0.9224\n",
            "Iteration 269: loss = 0.2768, accuracy = 0.9224\n",
            "Iteration 270: loss = 0.2767, accuracy = 0.9225\n",
            "Iteration 271: loss = 0.2766, accuracy = 0.9225\n",
            "Iteration 272: loss = 0.2765, accuracy = 0.9226\n",
            "Iteration 273: loss = 0.2764, accuracy = 0.9226\n",
            "Iteration 274: loss = 0.2763, accuracy = 0.9226\n",
            "Iteration 275: loss = 0.2762, accuracy = 0.9227\n",
            "Iteration 276: loss = 0.2761, accuracy = 0.9227\n",
            "Iteration 277: loss = 0.2760, accuracy = 0.9227\n",
            "Iteration 278: loss = 0.2759, accuracy = 0.9227\n",
            "Iteration 279: loss = 0.2758, accuracy = 0.9227\n",
            "Iteration 280: loss = 0.2757, accuracy = 0.9227\n",
            "Iteration 281: loss = 0.2756, accuracy = 0.9227\n",
            "Iteration 282: loss = 0.2755, accuracy = 0.9227\n",
            "Iteration 283: loss = 0.2754, accuracy = 0.9227\n",
            "Iteration 284: loss = 0.2753, accuracy = 0.9227\n",
            "Iteration 285: loss = 0.2752, accuracy = 0.9227\n",
            "Iteration 286: loss = 0.2751, accuracy = 0.9227\n",
            "Iteration 287: loss = 0.2751, accuracy = 0.9228\n",
            "Iteration 288: loss = 0.2750, accuracy = 0.9228\n",
            "Iteration 289: loss = 0.2749, accuracy = 0.9228\n",
            "Iteration 290: loss = 0.2748, accuracy = 0.9228\n",
            "Iteration 291: loss = 0.2747, accuracy = 0.9229\n",
            "Iteration 292: loss = 0.2746, accuracy = 0.9229\n",
            "Iteration 293: loss = 0.2745, accuracy = 0.9229\n",
            "Iteration 294: loss = 0.2744, accuracy = 0.9229\n",
            "Iteration 295: loss = 0.2743, accuracy = 0.9230\n",
            "Iteration 296: loss = 0.2742, accuracy = 0.9230\n",
            "Iteration 297: loss = 0.2741, accuracy = 0.9230\n",
            "Iteration 298: loss = 0.2741, accuracy = 0.9230\n",
            "Iteration 299: loss = 0.2740, accuracy = 0.9230\n",
            "Iteration 300: loss = 0.2739, accuracy = 0.9230\n",
            "Iteration 301: loss = 0.2738, accuracy = 0.9231\n",
            "Iteration 302: loss = 0.2737, accuracy = 0.9231\n",
            "Iteration 303: loss = 0.2736, accuracy = 0.9231\n",
            "Iteration 304: loss = 0.2735, accuracy = 0.9232\n",
            "Iteration 305: loss = 0.2735, accuracy = 0.9233\n",
            "Iteration 306: loss = 0.2734, accuracy = 0.9233\n",
            "Iteration 307: loss = 0.2733, accuracy = 0.9234\n",
            "Iteration 308: loss = 0.2732, accuracy = 0.9234\n",
            "Iteration 309: loss = 0.2731, accuracy = 0.9234\n",
            "Iteration 310: loss = 0.2730, accuracy = 0.9234\n",
            "Iteration 311: loss = 0.2730, accuracy = 0.9234\n",
            "Iteration 312: loss = 0.2729, accuracy = 0.9235\n",
            "Iteration 313: loss = 0.2728, accuracy = 0.9235\n",
            "Iteration 314: loss = 0.2727, accuracy = 0.9235\n",
            "Iteration 315: loss = 0.2726, accuracy = 0.9235\n",
            "Iteration 316: loss = 0.2725, accuracy = 0.9235\n",
            "Iteration 317: loss = 0.2725, accuracy = 0.9236\n",
            "Iteration 318: loss = 0.2724, accuracy = 0.9236\n",
            "Iteration 319: loss = 0.2723, accuracy = 0.9236\n",
            "Iteration 320: loss = 0.2722, accuracy = 0.9236\n",
            "Iteration 321: loss = 0.2721, accuracy = 0.9236\n",
            "Iteration 322: loss = 0.2721, accuracy = 0.9237\n",
            "Iteration 323: loss = 0.2720, accuracy = 0.9237\n",
            "Iteration 324: loss = 0.2719, accuracy = 0.9237\n",
            "Iteration 325: loss = 0.2718, accuracy = 0.9238\n",
            "Iteration 326: loss = 0.2718, accuracy = 0.9239\n",
            "Iteration 327: loss = 0.2717, accuracy = 0.9239\n",
            "Iteration 328: loss = 0.2716, accuracy = 0.9239\n",
            "Iteration 329: loss = 0.2715, accuracy = 0.9239\n",
            "Iteration 330: loss = 0.2714, accuracy = 0.9239\n",
            "Iteration 331: loss = 0.2714, accuracy = 0.9239\n",
            "Iteration 332: loss = 0.2713, accuracy = 0.9240\n",
            "Iteration 333: loss = 0.2712, accuracy = 0.9240\n",
            "Iteration 334: loss = 0.2711, accuracy = 0.9240\n",
            "Iteration 335: loss = 0.2711, accuracy = 0.9240\n",
            "Iteration 336: loss = 0.2710, accuracy = 0.9241\n",
            "Iteration 337: loss = 0.2709, accuracy = 0.9241\n",
            "Iteration 338: loss = 0.2708, accuracy = 0.9242\n",
            "Iteration 339: loss = 0.2708, accuracy = 0.9242\n",
            "Iteration 340: loss = 0.2707, accuracy = 0.9242\n",
            "Iteration 341: loss = 0.2706, accuracy = 0.9242\n",
            "Iteration 342: loss = 0.2705, accuracy = 0.9242\n",
            "Iteration 343: loss = 0.2705, accuracy = 0.9242\n",
            "Iteration 344: loss = 0.2704, accuracy = 0.9243\n",
            "Iteration 345: loss = 0.2703, accuracy = 0.9243\n",
            "Iteration 346: loss = 0.2703, accuracy = 0.9243\n",
            "Iteration 347: loss = 0.2702, accuracy = 0.9244\n",
            "Iteration 348: loss = 0.2701, accuracy = 0.9244\n",
            "Iteration 349: loss = 0.2700, accuracy = 0.9245\n",
            "Iteration 350: loss = 0.2700, accuracy = 0.9245\n",
            "Iteration 351: loss = 0.2699, accuracy = 0.9245\n",
            "Iteration 352: loss = 0.2698, accuracy = 0.9245\n",
            "Iteration 353: loss = 0.2698, accuracy = 0.9244\n",
            "Iteration 354: loss = 0.2697, accuracy = 0.9245\n",
            "Iteration 355: loss = 0.2696, accuracy = 0.9245\n",
            "Iteration 356: loss = 0.2696, accuracy = 0.9245\n",
            "Iteration 357: loss = 0.2695, accuracy = 0.9245\n",
            "Iteration 358: loss = 0.2694, accuracy = 0.9245\n",
            "Iteration 359: loss = 0.2694, accuracy = 0.9246\n",
            "Iteration 360: loss = 0.2693, accuracy = 0.9246\n",
            "Iteration 361: loss = 0.2692, accuracy = 0.9246\n",
            "Iteration 362: loss = 0.2691, accuracy = 0.9246\n",
            "Iteration 363: loss = 0.2691, accuracy = 0.9246\n",
            "Iteration 364: loss = 0.2690, accuracy = 0.9246\n",
            "Iteration 365: loss = 0.2689, accuracy = 0.9247\n",
            "Iteration 366: loss = 0.2689, accuracy = 0.9247\n",
            "Iteration 367: loss = 0.2688, accuracy = 0.9247\n",
            "Iteration 368: loss = 0.2687, accuracy = 0.9247\n",
            "Iteration 369: loss = 0.2687, accuracy = 0.9248\n",
            "Iteration 370: loss = 0.2686, accuracy = 0.9248\n",
            "Iteration 371: loss = 0.2686, accuracy = 0.9249\n",
            "Iteration 372: loss = 0.2685, accuracy = 0.9249\n",
            "Iteration 373: loss = 0.2684, accuracy = 0.9249\n",
            "Iteration 374: loss = 0.2684, accuracy = 0.9249\n",
            "Iteration 375: loss = 0.2683, accuracy = 0.9250\n",
            "Iteration 376: loss = 0.2682, accuracy = 0.9250\n",
            "Iteration 377: loss = 0.2682, accuracy = 0.9250\n",
            "Iteration 378: loss = 0.2681, accuracy = 0.9250\n",
            "Iteration 379: loss = 0.2680, accuracy = 0.9251\n",
            "Iteration 380: loss = 0.2680, accuracy = 0.9251\n",
            "Iteration 381: loss = 0.2679, accuracy = 0.9251\n",
            "Iteration 382: loss = 0.2678, accuracy = 0.9251\n",
            "Iteration 383: loss = 0.2678, accuracy = 0.9251\n",
            "Iteration 384: loss = 0.2677, accuracy = 0.9251\n",
            "Iteration 385: loss = 0.2677, accuracy = 0.9251\n",
            "Iteration 386: loss = 0.2676, accuracy = 0.9251\n",
            "Iteration 387: loss = 0.2675, accuracy = 0.9252\n",
            "Iteration 388: loss = 0.2675, accuracy = 0.9252\n",
            "Iteration 389: loss = 0.2674, accuracy = 0.9252\n",
            "Iteration 390: loss = 0.2674, accuracy = 0.9252\n",
            "Iteration 391: loss = 0.2673, accuracy = 0.9252\n",
            "Iteration 392: loss = 0.2672, accuracy = 0.9253\n",
            "Iteration 393: loss = 0.2672, accuracy = 0.9253\n",
            "Iteration 394: loss = 0.2671, accuracy = 0.9253\n",
            "Iteration 395: loss = 0.2670, accuracy = 0.9253\n",
            "Iteration 396: loss = 0.2670, accuracy = 0.9254\n",
            "Iteration 397: loss = 0.2669, accuracy = 0.9254\n",
            "Iteration 398: loss = 0.2669, accuracy = 0.9254\n",
            "Iteration 399: loss = 0.2668, accuracy = 0.9254\n",
            "Iteration 400: loss = 0.2668, accuracy = 0.9255\n",
            "Iteration 401: loss = 0.2667, accuracy = 0.9255\n",
            "Iteration 402: loss = 0.2666, accuracy = 0.9255\n",
            "Iteration 403: loss = 0.2666, accuracy = 0.9256\n",
            "Iteration 404: loss = 0.2665, accuracy = 0.9256\n",
            "Iteration 405: loss = 0.2665, accuracy = 0.9256\n",
            "Iteration 406: loss = 0.2664, accuracy = 0.9257\n",
            "Iteration 407: loss = 0.2663, accuracy = 0.9257\n",
            "Iteration 408: loss = 0.2663, accuracy = 0.9257\n",
            "Iteration 409: loss = 0.2662, accuracy = 0.9258\n",
            "Iteration 410: loss = 0.2662, accuracy = 0.9258\n",
            "Iteration 411: loss = 0.2661, accuracy = 0.9258\n",
            "Iteration 412: loss = 0.2661, accuracy = 0.9258\n",
            "Iteration 413: loss = 0.2660, accuracy = 0.9259\n",
            "Iteration 414: loss = 0.2659, accuracy = 0.9259\n",
            "Iteration 415: loss = 0.2659, accuracy = 0.9259\n",
            "Iteration 416: loss = 0.2658, accuracy = 0.9258\n",
            "Iteration 417: loss = 0.2658, accuracy = 0.9259\n",
            "Iteration 418: loss = 0.2657, accuracy = 0.9259\n",
            "Iteration 419: loss = 0.2657, accuracy = 0.9259\n",
            "Iteration 420: loss = 0.2656, accuracy = 0.9259\n",
            "Iteration 421: loss = 0.2656, accuracy = 0.9259\n",
            "Iteration 422: loss = 0.2655, accuracy = 0.9259\n",
            "Iteration 423: loss = 0.2654, accuracy = 0.9259\n",
            "Iteration 424: loss = 0.2654, accuracy = 0.9259\n",
            "Iteration 425: loss = 0.2653, accuracy = 0.9260\n",
            "Iteration 426: loss = 0.2653, accuracy = 0.9260\n",
            "Iteration 427: loss = 0.2652, accuracy = 0.9260\n",
            "Iteration 428: loss = 0.2652, accuracy = 0.9260\n",
            "Iteration 429: loss = 0.2651, accuracy = 0.9259\n",
            "Iteration 430: loss = 0.2651, accuracy = 0.9260\n",
            "Iteration 431: loss = 0.2650, accuracy = 0.9260\n",
            "Iteration 432: loss = 0.2650, accuracy = 0.9260\n",
            "Iteration 433: loss = 0.2649, accuracy = 0.9260\n",
            "Iteration 434: loss = 0.2648, accuracy = 0.9261\n",
            "Iteration 435: loss = 0.2648, accuracy = 0.9261\n",
            "Iteration 436: loss = 0.2647, accuracy = 0.9261\n",
            "Iteration 437: loss = 0.2647, accuracy = 0.9262\n",
            "Iteration 438: loss = 0.2646, accuracy = 0.9262\n",
            "Iteration 439: loss = 0.2646, accuracy = 0.9262\n",
            "Iteration 440: loss = 0.2645, accuracy = 0.9262\n",
            "Iteration 441: loss = 0.2645, accuracy = 0.9262\n",
            "Iteration 442: loss = 0.2644, accuracy = 0.9262\n",
            "Iteration 443: loss = 0.2644, accuracy = 0.9262\n",
            "Iteration 444: loss = 0.2643, accuracy = 0.9262\n",
            "Iteration 445: loss = 0.2643, accuracy = 0.9263\n",
            "Iteration 446: loss = 0.2642, accuracy = 0.9263\n",
            "Iteration 447: loss = 0.2642, accuracy = 0.9263\n",
            "Iteration 448: loss = 0.2641, accuracy = 0.9263\n",
            "Iteration 449: loss = 0.2641, accuracy = 0.9263\n",
            "Iteration 450: loss = 0.2640, accuracy = 0.9263\n",
            "Iteration 451: loss = 0.2640, accuracy = 0.9263\n",
            "Iteration 452: loss = 0.2639, accuracy = 0.9264\n",
            "Iteration 453: loss = 0.2639, accuracy = 0.9264\n",
            "Iteration 454: loss = 0.2638, accuracy = 0.9264\n",
            "Iteration 455: loss = 0.2638, accuracy = 0.9264\n",
            "Iteration 456: loss = 0.2637, accuracy = 0.9264\n",
            "Iteration 457: loss = 0.2637, accuracy = 0.9264\n",
            "Iteration 458: loss = 0.2636, accuracy = 0.9264\n",
            "Iteration 459: loss = 0.2636, accuracy = 0.9265\n",
            "Iteration 460: loss = 0.2635, accuracy = 0.9265\n",
            "Iteration 461: loss = 0.2635, accuracy = 0.9265\n",
            "Iteration 462: loss = 0.2634, accuracy = 0.9266\n",
            "Iteration 463: loss = 0.2634, accuracy = 0.9266\n",
            "Iteration 464: loss = 0.2633, accuracy = 0.9266\n",
            "Iteration 465: loss = 0.2633, accuracy = 0.9266\n",
            "Iteration 466: loss = 0.2632, accuracy = 0.9266\n",
            "Iteration 467: loss = 0.2632, accuracy = 0.9266\n",
            "Iteration 468: loss = 0.2631, accuracy = 0.9266\n",
            "Iteration 469: loss = 0.2631, accuracy = 0.9266\n",
            "Iteration 470: loss = 0.2630, accuracy = 0.9266\n",
            "Iteration 471: loss = 0.2630, accuracy = 0.9266\n",
            "Iteration 472: loss = 0.2629, accuracy = 0.9266\n",
            "Iteration 473: loss = 0.2629, accuracy = 0.9267\n",
            "Iteration 474: loss = 0.2628, accuracy = 0.9267\n",
            "Iteration 475: loss = 0.2628, accuracy = 0.9267\n",
            "Iteration 476: loss = 0.2627, accuracy = 0.9267\n",
            "Iteration 477: loss = 0.2627, accuracy = 0.9268\n",
            "Iteration 478: loss = 0.2627, accuracy = 0.9268\n",
            "Iteration 479: loss = 0.2626, accuracy = 0.9268\n",
            "Iteration 480: loss = 0.2626, accuracy = 0.9268\n",
            "Iteration 481: loss = 0.2625, accuracy = 0.9268\n",
            "Iteration 482: loss = 0.2625, accuracy = 0.9268\n",
            "Iteration 483: loss = 0.2624, accuracy = 0.9269\n",
            "Iteration 484: loss = 0.2624, accuracy = 0.9269\n",
            "Iteration 485: loss = 0.2623, accuracy = 0.9269\n",
            "Iteration 486: loss = 0.2623, accuracy = 0.9269\n",
            "Iteration 487: loss = 0.2622, accuracy = 0.9269\n",
            "Iteration 488: loss = 0.2622, accuracy = 0.9269\n",
            "Iteration 489: loss = 0.2621, accuracy = 0.9269\n",
            "Iteration 490: loss = 0.2621, accuracy = 0.9270\n",
            "Iteration 491: loss = 0.2621, accuracy = 0.9270\n",
            "Iteration 492: loss = 0.2620, accuracy = 0.9270\n",
            "Iteration 493: loss = 0.2620, accuracy = 0.9271\n",
            "Iteration 494: loss = 0.2619, accuracy = 0.9271\n",
            "Iteration 495: loss = 0.2619, accuracy = 0.9271\n",
            "Iteration 496: loss = 0.2618, accuracy = 0.9271\n",
            "Iteration 497: loss = 0.2618, accuracy = 0.9272\n",
            "Iteration 498: loss = 0.2617, accuracy = 0.9272\n",
            "Iteration 499: loss = 0.2617, accuracy = 0.9272\n",
            "Iteration 500: loss = 0.2617, accuracy = 0.9272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = model.predict(x_test)"
      ],
      "metadata": {
        "id": "pzQCMR9zRwBx"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('The empirical risk when using the missclassification loss function =', np.count_nonzero(y_hat - y_test) / len(y_test))\n",
        "print('The empirical risk when using the squared error loss function =', sum((y_hat - y_test)**2) / len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbIYYteJTYxG",
        "outputId": "42168868-2464-442d-eb22-3ab0a5cfe575"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The empirical risk when using the missclassification loss function = 0.0763\n",
            "The empirical risk when using the squared error loss function = 1.3825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tO-mOVUAUr0W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}